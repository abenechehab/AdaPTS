{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pris4PSuayFX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwTRhv9yayFZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "available = torch.cuda.is_available()\n",
    "curr_device = torch.cuda.current_device()\n",
    "device = torch.device(\"cuda:0\" if available else \"cpu\")\n",
    "device_count = torch.cuda.device_count()\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "print(f\"Cuda available: {available}\")\n",
    "print(f\"Current device: {curr_device}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Device count: {device_count}\")\n",
    "print(f\"Device name: {device_name}\")\n",
    "\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zM0BFFPGayFa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf_pR4f4ayFb"
   },
   "outputs": [],
   "source": [
    "mean = [0, 0]\n",
    "cov = [[1, 0.8], [0.8, 1]]\n",
    "\n",
    "X = np.random.multivariate_normal(mean, cov, 5000)\n",
    "\n",
    "temp = X.T\n",
    "plt.plot(temp[0], temp[1], \"x\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKEwgrsrayFb"
   },
   "outputs": [],
   "source": [
    "class Dataset2D(Dataset):\n",
    "    def __init__(self, data):\n",
    "        mean = [0, 0]\n",
    "        cov = [[1, 0.8], [0.8, 1]]\n",
    "        self.size = len(data)\n",
    "        self.origX = data\n",
    "        self.X = torch.tensor(self.origX).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D745gjtZayFc"
   },
   "outputs": [],
   "source": [
    "def backbone(input_width, network_width=10):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_width, network_width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(network_width, network_width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(network_width, network_width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(network_width, input_width),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "\n",
    "\n",
    "class NormalizingFlow2D(nn.Module):\n",
    "    def __init__(self, num_coupling, width):\n",
    "        super(NormalizingFlow2D, self).__init__()\n",
    "        self.num_coupling = num_coupling\n",
    "        self.s = nn.ModuleList([backbone(1, width) for x in range(num_coupling)])\n",
    "        self.t = nn.ModuleList([backbone(1, width) for x in range(num_coupling)])\n",
    "\n",
    "        # Learnable scaling parameters for outputs of S\n",
    "        self.s_scale = torch.nn.Parameter(torch.randn(num_coupling))\n",
    "        self.s_scale.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if model.training:\n",
    "            s_vals = []\n",
    "            y1, y2 = x[:, :1], x[:, 1:]\n",
    "            for i in range(self.num_coupling):\n",
    "                # Alternating which var gets transformed\n",
    "                if i % 2 == 0:\n",
    "                    x1, x2 = y1, y2\n",
    "                    y1 = x1\n",
    "                    s = self.s_scale[i] * self.s[i](x1)\n",
    "                    y2 = torch.exp(s) * x2 + self.t[i](x1)\n",
    "                else:\n",
    "                    x1, x2 = y1, y2\n",
    "                    y2 = x2\n",
    "                    s = self.s_scale[i] * self.s[i](x2)\n",
    "                    y1 = torch.exp(s) * x1 + self.t[i](x2)\n",
    "                s_vals.append(s)\n",
    "\n",
    "            # Return outputs and vars needed for determinant\n",
    "            return torch.cat([y1, y2], 1), torch.cat(s_vals)\n",
    "        else:\n",
    "            # Assume x is sampled from random Gaussians\n",
    "            x1, x2 = x[:, :1], x[:, 1:]\n",
    "\n",
    "            for i in reversed(range(self.num_coupling)):\n",
    "                # Alternating which var gets transformed\n",
    "                if i % 2 == 0:\n",
    "                    y1, y2 = x1, x2\n",
    "                    x1 = y1\n",
    "                    s = self.s_scale[i] * self.s[i](y1)\n",
    "                    x2 = (y2 - self.t[i](y1)) * torch.exp(-s)\n",
    "                else:\n",
    "                    y1, y2 = x1, x2\n",
    "                    x2 = y2\n",
    "                    s = self.s_scale[i] * self.s[i](y2)\n",
    "                    x1 = (y1 - self.t[i](y2)) * torch.exp(-s)\n",
    "\n",
    "            return torch.cat([x1, x2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0krPhEWayFd"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, report_iters=10):\n",
    "    size = len(dataloader)\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Transfer to GPU\n",
    "        X = X.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        y, s = model(X)\n",
    "        loss = loss_fn(y, s, batch_size)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % report_iters == 0:\n",
    "            loss, current = loss.item(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X = X.to(device)\n",
    "            y, s = model(X)\n",
    "            test_loss += loss_fn(y, s, batch_size)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RGBWzGKayFe"
   },
   "outputs": [],
   "source": [
    "def loss_fn(y, s, batch_size):\n",
    "    # -log(zero-mean gaussian) + log determinant\n",
    "    # -log p_x = log(pz(f(x))) + log(det(\\partial f/\\partial x))\n",
    "    # -log p_x = 0.5 * y**2 + s1 + s2\n",
    "    logpx = -torch.sum(0.5 * y**2)\n",
    "    det = torch.sum(s)\n",
    "\n",
    "    ret = -(logpx + det)\n",
    "    return torch.div(ret, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yROSKwH9ayFe"
   },
   "source": [
    "# 2D Bivariate Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5scTjFmayFf"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "epochs = 10\n",
    "\n",
    "model = NormalizingFlow2D(16, 10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.8], [0.8, 1]]\n",
    "training_data = Dataset2D(np.random.multivariate_normal(mean, cov, 20000))\n",
    "test_data = Dataset2D(np.random.multivariate_normal(mean, cov, 5000))\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7oQp5TBayFg"
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# subplot1\n",
    "ax1.set_title(\"Original\")\n",
    "ax1.set_xlabel(\"x1\")\n",
    "ax1.set_ylabel(\"x2\")\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax1.set_ylim(-4, 4)\n",
    "\n",
    "data = training_data.origX.T\n",
    "ax1.plot(data[0], data[1], \"x\", color=\"blue\")\n",
    "\n",
    "# subplot2\n",
    "model.eval()\n",
    "ax2.set_title(\"NF Sampled\")\n",
    "ax2.set_xlabel(\"x1\")\n",
    "ax2.set_ylabel(\"x2\")\n",
    "ax2.set_xlim(-4, 4)\n",
    "ax2.set_ylim(-4, 4)\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "with torch.no_grad():\n",
    "    X = torch.Tensor(np.random.multivariate_normal(mean, cov, 20000)).to(device)\n",
    "    Y = model(X)\n",
    "samples = Y.cpu().numpy().T\n",
    "ax2.plot(samples[0], samples[1], \"x\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyCFEWc0ayFh"
   },
   "source": [
    "# Scikit 2D Artificial Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLv8H-k-ayFh"
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "n_samples = 10000\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)[0]\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)[0]\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)[0]\n",
    "no_structure = np.random.rand(n_samples, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Zfe2NW1ayFi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "epochs = 100\n",
    "\n",
    "datasets = [\n",
    "    (\"Noisy Circles\", noisy_circles),\n",
    "    (\"Noisy Moons\", noisy_moons),\n",
    "    (\"Blobs\", blobs),\n",
    "    (\"Random\", no_structure),\n",
    "]\n",
    "models = []\n",
    "orig_data = []\n",
    "for name, dataset in datasets:\n",
    "    model = NormalizingFlow2D(32, 20).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    training_data = Dataset2D(dataset)\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(\n",
    "        f\"\\n======================================\\n{name}\\n======================================\"\n",
    "    )\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, report_iters=1)\n",
    "\n",
    "    models.append(model)\n",
    "    orig_data.append(training_data.origX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWX-oI4qayFi"
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\n",
    "for i, ((name, _), ax) in enumerate(zip(datasets, [ax1, ax2, ax3, ax4])):\n",
    "    # subplot1\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "\n",
    "    data = orig_data[i].T\n",
    "    ax.plot(data[0], data[1], \"x\", color=\"blue\")\n",
    "\n",
    "    # subplot2\n",
    "    model = models[i].eval()\n",
    "    mean = [0, 0]\n",
    "    cov = [[1, 0], [0, 1]]\n",
    "    with torch.no_grad():\n",
    "        X = torch.Tensor(np.random.multivariate_normal(mean, cov, 2000)).to(device)\n",
    "        Y = model(X)\n",
    "    samples = Y.cpu().numpy().T\n",
    "    ax.plot(samples[0], samples[1], \"x\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cX7UdkCayFj"
   },
   "source": [
    "# 2022-01-08\n",
    "\n",
    "* Looks like upping the experimetn works relatively well for the others as well:\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "* Removing tanh and associated learnable scaling factor (just have identity outputs on backbone looks similar).\n",
    "* It also turns out I didn't end up using the residual connection (made the class but never used it)\n",
    "* I added back tanh because it looks like it helps stabilize it.  I got NaNs in some of the runs, most likely because of the `exp(s)`, which can blow up the problem.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1cK91W6ayFj"
   },
   "source": [
    "# 2022-01-06\n",
    "\n",
    "* Changed to use backbone with tanh * learnable scaling factor\n",
    "* Added a residual connection\n",
    "* Experimented with a few variations, looks like coupling(32), width(20) works with LR=0.001 and epochs=100 (main thing was giving it enough epochs to run with a small enough LR)\n",
    "* More epochs gave a reasonable noisy moon too\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Check this settings on the other ones too (takes longer to train so I was impatient)\n",
    "* Will check if the tanh * learnable and/or residual did anything, questionable\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnWgqUWCayFj"
   },
   "source": [
    "# 2022-01-04\n",
    "\n",
    "* Tried increasing coupling layers (16) + width(20) but still can't get a clean break\n",
    "* Can sort of generate Noisy Moons and Blobs, but Noisy circles looks mostly like noise (Random looks ok except for periphery\n",
    "* Change to tanh * scaling constant instead for s?  That's what they used in the paper\n",
    "* Interestingly, it would make it much easier if I could map it to more than 2D, but in the paper they have an example where somehow they are able to do it, but it might have been an illustration and not a real run they did\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mywgivoayFk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

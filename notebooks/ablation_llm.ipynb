{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "from dicl import dicl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will be using expert trajectories from the HalfCheetah Mujoco environment for our demo. The dataset is provided in `src/dicl/data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetah\"\n",
    "n_actions = 6  # number of actions in the HalfCheetah system\n",
    "n_observations = 17  # number of observations in the HalfCheetah system\n",
    "data_label = \"expert\"\n",
    "data_path = Path(\"../src\") / \"dicl\" / \"data\" / f\"D4RL_{env_name}_{data_label}.csv\"\n",
    "\n",
    "# ICL parameters\n",
    "context_length = 300\n",
    "rescale_factor = 7.0\n",
    "up_shift = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pick DICL(s) or DICL(s,a) method through the number of features (choose `n_observations` for vICL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use DICL-(s) or vICL, set include_actions to False.\n",
    "# to use DICL-(s,a), set include_actions to True\n",
    "include_actions = False\n",
    "if include_actions:\n",
    "    n_features = n_observations + n_actions\n",
    "else:\n",
    "    n_features = n_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sample an episode and extract an in-context trajectory `(n_timestamps, n_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to get a sample episode\n",
    "X = pd.read_csv(data_path, index_col=0)\n",
    "X = X.values.astype(\"float\")\n",
    "\n",
    "# find episodes beginnings. the restart column is equal to 1 at the start of\n",
    "# an episode, 0 otherwise.\n",
    "restart_index = n_observations + n_actions + 1\n",
    "restarts = X[:, restart_index]\n",
    "episode_starts = np.where(restarts)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instantiate DICL\n",
    "* Choose the number of components for PCA (set to half here)\n",
    "* Dor vICL n_components has to be equal to n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm:   0%|                                                                                                         | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llm:  20%|███████████████████▍                                                                             | 1/5 [00:40<02:42, 40.73s/it]\n",
      "Loading checkpoint shards:   0%|                                                                                   | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "llm_list = [\n",
    "    \"/mnt/vdb/hugguingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/5d853ed7d16ac794afa8f5c9c7f59f4e9c950954\"\n",
    "]\n",
    "llm_list += [\n",
    "    \"/mnt/vdb/hugguingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/43fa890183375f5f69cb9646f29aa99ef3207c22\"\n",
    "]\n",
    "llm_list += [\n",
    "    \"/mnt/vdb/hugguingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/8d10549bcf802355f2d6203a33ed27e81b15b9e5\"\n",
    "]\n",
    "llm_list += [\n",
    "    \"/home/gpaolo/nas_2/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/\"\n",
    "]\n",
    "llm_list += [\n",
    "    \"/mnt/vdb/hugguingface/hub/models--meta-llama--Llama-3.1-70B/snapshots/349b2ddb53ce8f2849a6c168a81980ab25258dac/\"\n",
    "]\n",
    "\n",
    "# to use vICL, set vanilla_icl to True.\n",
    "# to use DICL-(s,a) or DICL-(s), set vanilla_icl to False\n",
    "vanilla_icl = True\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "n_episodes = 5\n",
    "selected_episodes = np.random.choice(episode_starts, (n_episodes,))\n",
    "\n",
    "for llm_model in tqdm(llm_list, desc='llm'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        llm_model,\n",
    "        use_fast=False,\n",
    "    )\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        llm_model,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    for episode in selected_episodes:\n",
    "        time_series = X[episode : episode + context_length, :n_features]\n",
    "        result_dict[episode] = {}\n",
    "    \n",
    "        if vanilla_icl:\n",
    "            DICL = dicl.vICL(\n",
    "                n_features=n_features,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                rescale_factor=rescale_factor,\n",
    "                up_shift=up_shift,\n",
    "            )\n",
    "        else:\n",
    "            DICL = dicl.DICL_PCA(\n",
    "                n_features=n_features,\n",
    "                n_components=int(n_features / 2),\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                rescale_factor=rescale_factor,\n",
    "                up_shift=up_shift,\n",
    "            )\n",
    "    \n",
    "        DICL.fit_disentangler(X=time_series)\n",
    "    \n",
    "        mean, mode, lb, ub = DICL.predict_single_step(X=time_series)\n",
    "    \n",
    "        # print metrics\n",
    "        burnin = 0\n",
    "        single_step_metrics = DICL.compute_metrics(burnin=burnin)\n",
    "    \n",
    "        result_dict[episode][llm_model.split(\"--\")[2].split('/')[0]] = single_step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DICL]",
   "language": "python",
   "name": "conda-env-DICL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

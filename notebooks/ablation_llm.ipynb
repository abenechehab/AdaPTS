{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "from dicl import dicl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will be using expert trajectories from the HalfCheetah Mujoco environment for our demo. The dataset is provided in `src/dicl/data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetah\"\n",
    "n_actions = 6  # number of actions in the HalfCheetah system\n",
    "n_observations = 17  # number of observations in the HalfCheetah system\n",
    "data_label = \"expert\"\n",
    "data_path = Path(\"../src\") / \"dicl\" / \"data\" / f\"D4RL_{env_name}_{data_label}.csv\"\n",
    "\n",
    "# ICL parameters\n",
    "context_length = 300\n",
    "rescale_factor = 7.0\n",
    "up_shift = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pick DICL(s) or DICL(s,a) method through the number of features (choose `n_observations` for vICL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use DICL-(s) or vICL, set include_actions to False.\n",
    "# to use DICL-(s,a), set include_actions to True\n",
    "include_actions = False\n",
    "if include_actions:\n",
    "    n_features = n_observations + n_actions\n",
    "else:\n",
    "    n_features = n_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sample an episode and extract an in-context trajectory `(n_timestamps, n_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to get a sample episode\n",
    "X = pd.read_csv(data_path, index_col=0)\n",
    "X = X.values.astype(\"float\")\n",
    "\n",
    "# find episodes beginnings. the restart column is equal to 1 at the start of\n",
    "# an episode, 0 otherwise.\n",
    "restart_index = n_observations + n_actions + 1\n",
    "restarts = X[:, restart_index]\n",
    "episode_starts = np.where(restarts)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instantiate DICL\n",
    "* Choose the number of components for PCA (set to half here)\n",
    "* Dor vICL n_components has to be equal to n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm:   0%|                                                                                                         | 0/5 [00:00<?, ?it/s]2024-10-17 14:58:02.741111: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-17 14:58:02.759566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-17 14:58:02.780192: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-17 14:58:02.786481: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-17 14:58:02.803067: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 14:58:03.632305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "llm:  20%|███████████████████▍                                                                             | 1/5 [00:46<03:05, 46.38s/it]\n",
      "Loading checkpoint shards:   0%|                                                                                   | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████████████████████████████████████▌                                     | 1/2 [00:02<00:02,  2.40s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.78s/it]\u001b[A\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llm:  40%|██████████████████████████████████████▊                                                          | 2/5 [02:03<03:12, 64.18s/it]\n",
      "Loading checkpoint shards:   0%|                                                                                   | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  25%|██████████████████▊                                                        | 1/4 [00:02<00:07,  2.43s/it]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████████████████████████████████████▌                                     | 2/4 [00:05<00:05,  2.73s/it]\u001b[A\n",
      "Loading checkpoint shards:  75%|████████████████████████████████████████████████████████▎                  | 3/4 [00:07<00:02,  2.51s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.10s/it]\u001b[A\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llm:  60%|█████████████████████████████████████████████████████████▌                                      | 3/5 [04:30<03:24, 102.27s/it]\n",
      "Loading checkpoint shards:   0%|                                                                                   | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  25%|██████████████████▊                                                        | 1/4 [00:02<00:08,  2.92s/it]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████████████████████████████████████▌                                     | 2/4 [00:05<00:05,  2.66s/it]\u001b[A\n",
      "Loading checkpoint shards:  75%|████████████████████████████████████████████████████████▎                  | 3/4 [00:07<00:02,  2.63s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.15s/it]\u001b[A\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llm:  80%|████████████████████████████████████████████████████████████████████████████▊                   | 4/5 [06:59<02:00, 120.50s/it]\n",
      "Loading checkpoint shards:   0%|                                                                                  | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:   3%|██▍                                                                       | 1/30 [00:02<01:00,  2.10s/it]\u001b[A\n",
      "Loading checkpoint shards:   7%|████▉                                                                     | 2/30 [00:04<01:06,  2.38s/it]\u001b[A\n",
      "Loading checkpoint shards:  10%|███████▍                                                                  | 3/30 [00:06<01:02,  2.31s/it]\u001b[A\n",
      "Loading checkpoint shards:  13%|█████████▊                                                                | 4/30 [00:09<01:03,  2.45s/it]\u001b[A\n",
      "Loading checkpoint shards:  17%|████████████▎                                                             | 5/30 [00:11<00:59,  2.38s/it]\u001b[A\n",
      "Loading checkpoint shards:  20%|██████████████▊                                                           | 6/30 [00:14<00:57,  2.40s/it]\u001b[A\n",
      "Loading checkpoint shards:  23%|█████████████████▎                                                        | 7/30 [00:16<00:53,  2.33s/it]\u001b[A\n",
      "Loading checkpoint shards:  27%|███████████████████▋                                                      | 8/30 [00:19<00:54,  2.47s/it]\u001b[A\n",
      "Loading checkpoint shards:  30%|██████████████████████▏                                                   | 9/30 [00:21<00:50,  2.38s/it]\u001b[A\n",
      "Loading checkpoint shards:  33%|████████████████████████▎                                                | 10/30 [00:23<00:48,  2.44s/it]\u001b[A\n",
      "Loading checkpoint shards:  37%|██████████████████████████▊                                              | 11/30 [00:26<00:44,  2.33s/it]\u001b[A\n",
      "Loading checkpoint shards:  40%|█████████████████████████████▏                                           | 12/30 [00:28<00:40,  2.26s/it]\u001b[A\n",
      "Loading checkpoint shards:  43%|███████████████████████████████▋                                         | 13/30 [00:30<00:39,  2.30s/it]\u001b[A\n",
      "Loading checkpoint shards:  47%|██████████████████████████████████                                       | 14/30 [00:32<00:36,  2.26s/it]\u001b[A\n",
      "Loading checkpoint shards:  50%|████████████████████████████████████▌                                    | 15/30 [00:35<00:35,  2.35s/it]\u001b[A\n",
      "Loading checkpoint shards:  53%|██████████████████████████████████████▉                                  | 16/30 [00:37<00:32,  2.29s/it]\u001b[A\n",
      "Loading checkpoint shards:  57%|█████████████████████████████████████████▎                               | 17/30 [00:39<00:28,  2.23s/it]\u001b[A\n",
      "Loading checkpoint shards:  60%|███████████████████████████████████████████▊                             | 18/30 [00:41<00:27,  2.28s/it]\u001b[A\n",
      "Loading checkpoint shards:  63%|██████████████████████████████████████████████▏                          | 19/30 [00:44<00:24,  2.24s/it]\u001b[A\n",
      "Loading checkpoint shards:  67%|████████████████████████████████████████████████▋                        | 20/30 [00:46<00:23,  2.34s/it]\u001b[A\n",
      "Loading checkpoint shards:  70%|███████████████████████████████████████████████████                      | 21/30 [00:48<00:20,  2.25s/it]\u001b[A\n",
      "Loading checkpoint shards:  73%|█████████████████████████████████████████████████████▌                   | 22/30 [00:50<00:17,  2.20s/it]\u001b[A\n",
      "Loading checkpoint shards:  77%|███████████████████████████████████████████████████████▉                 | 23/30 [00:53<00:16,  2.30s/it]\u001b[A\n",
      "Loading checkpoint shards:  80%|██████████████████████████████████████████████████████████▍              | 24/30 [00:55<00:13,  2.26s/it]\u001b[A\n",
      "Loading checkpoint shards:  83%|████████████████████████████████████████████████████████████▊            | 25/30 [00:58<00:11,  2.35s/it]\u001b[A\n",
      "Loading checkpoint shards:  87%|███████████████████████████████████████████████████████████████▎         | 26/30 [01:00<00:09,  2.26s/it]\u001b[A\n",
      "Loading checkpoint shards:  90%|█████████████████████████████████████████████████████████████████▋       | 27/30 [01:02<00:06,  2.20s/it]\u001b[A\n",
      "Loading checkpoint shards:  93%|████████████████████████████████████████████████████████████████████▏    | 28/30 [01:04<00:04,  2.30s/it]\u001b[A\n",
      "Loading checkpoint shards:  97%|██████████████████████████████████████████████████████████████████████▌  | 29/30 [01:06<00:02,  2.24s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████| 30/30 [01:07<00:00,  2.25s/it]\u001b[A\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "llm: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [21:19<00:00, 255.98s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_list = [\n",
    "    \"/mnt/vdb/hugguingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/5d853ed7d16ac794afa8f5c9c7f59f4e9c950954\"\n",
    "]\n",
    "llm_list += [\n",
    "    \"/mnt/vdb/hugguingface/hub/models--meta-llama--Llama-3.2-3B/snapshots/43fa890183375f5f69cb9646f29aa99ef3207c22\"\n",
    "]\n",
    "llm_list += [\n",
    "    \"/mnt/vdb/hugguingface/hub/models--meta-llama--Llama-3.1-8B/snapshots/8d10549bcf802355f2d6203a33ed27e81b15b9e5\"\n",
    "]\n",
    "llm_list += [\n",
    "    \"/home/gpaolo/nas_2/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/\"\n",
    "]\n",
    "llm_list += [\n",
    "    \"/mnt/vdb/hugguingface/hub/models--meta-llama--Llama-3.1-70B/snapshots/349b2ddb53ce8f2849a6c168a81980ab25258dac/\"\n",
    "]\n",
    "\n",
    "# to use vICL, set vanilla_icl to True.\n",
    "# to use DICL-(s,a) or DICL-(s), set vanilla_icl to False\n",
    "vanilla_icl = True\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "n_episodes = 5\n",
    "selected_episodes = np.random.choice(episode_starts, (n_episodes,))\n",
    "\n",
    "for llm_model in tqdm(llm_list, desc='llm'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        llm_model,\n",
    "        use_fast=False,\n",
    "    )\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        llm_model,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    for episode in selected_episodes:\n",
    "        time_series = X[episode : episode + context_length, :n_features]\n",
    "        if episode not in result_dict.keys():\n",
    "            result_dict[episode] = {}\n",
    "    \n",
    "        if vanilla_icl:\n",
    "            DICL = dicl.vICL(\n",
    "                n_features=n_features,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                rescale_factor=rescale_factor,\n",
    "                up_shift=up_shift,\n",
    "            )\n",
    "        else:\n",
    "            DICL = dicl.DICL_PCA(\n",
    "                n_features=n_features,\n",
    "                n_components=int(n_features / 2),\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                rescale_factor=rescale_factor,\n",
    "                up_shift=up_shift,\n",
    "            )\n",
    "    \n",
    "        DICL.fit_disentangler(X=time_series)\n",
    "    \n",
    "        mean, mode, lb, ub = DICL.predict_single_step(X=time_series)\n",
    "    \n",
    "        # print metrics\n",
    "        burnin = 0\n",
    "        single_step_metrics = DICL.compute_metrics(burnin=burnin)\n",
    "    \n",
    "        result_dict[episode][llm_model.split(\"--\")[2].split('/')[0]] = single_step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"../artifacts\") / \"ablation_llm_single_step_vicl.pkl\"\n",
    "\n",
    "with open(save_path, \"wb\") as outfile:\n",
    "    pickle.dump(result_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "8000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(load_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[1;32m      3\u001b[0m     loaded_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(infile)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mloaded_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[0;31mKeyError\u001b[0m: 8000"
     ]
    }
   ],
   "source": [
    "load_path = Path(\"../artifacts\") / \"ablation_llm_single_step_vicl.pkl\"\n",
    "with open(load_path, \"rb\") as infile:\n",
    "    loaded_dict = pickle.load(infile)\n",
    "loaded_dict[8000].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DICL]",
   "language": "python",
   "name": "conda-env-DICL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
